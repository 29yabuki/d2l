{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9608e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a773333",
   "metadata": {},
   "source": [
    "## Scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e379e3",
   "metadata": {},
   "source": [
    "We denote:\n",
    "- scalars by _ordinary_ lowercase letters (e.g., $x$, $y$, and $z$).\n",
    "- the space of all continuous _real-valued_ scalars by $\\mathbb{R}$.\n",
    "\n",
    "$x \\in \\mathbb{R}$ is a way to say that $x$ is a real-valued scalar.\n",
    "\n",
    "Scalars are implemented as a one element tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b20a868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "x + y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1197f1",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d10a6",
   "metadata": {},
   "source": [
    "Vectors are fixed-length array of scalars. \n",
    "- Scalars are the _elements_ of a vector.\n",
    "- Vectors are denoted by _bold_ lowercase letters (e.g., $\\textbf{x}$, $\\textbf{y}$, and $\\textbf{z}$).\n",
    "\n",
    "Vectors are implemented as _first-order_ tensors. Said tensors can have arbitrary lengths, subject to memory limitations.\n",
    "\n",
    "\n",
    "When training a model, each vector represents an instance, with its components as relevant quantities. For instance, when creating a model predicting loan default risk, an applicant is represented as a vector with quantities like income, employment length, and number of previous defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12574026",
   "metadata": {},
   "source": [
    "By default, vectors are visualized vertically. In linear algebra, subscripts begin at $1$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\mathbf{x} = \n",
    "    \\begin{bmatrix}\n",
    "        x_{1}  \\\\ \n",
    "        \\vdots \\\\\n",
    "        x_{n}\n",
    "    \\end{bmatrix},\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "An element of a vector is referred using a subscript. $x_2$ represent the second element of $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb0d95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd8acf",
   "metadata": {},
   "source": [
    "In Python, vectors indices begin at $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1034e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011b61c",
   "metadata": {},
   "source": [
    "$\\textbf{x} \\in \\mathbb{R}^n$ indicate that a vector contains $n$ elements. Formally, $n$ is the _dimensionality_ of the vector.\n",
    "\n",
    "The dimensionality corresponds to a tensor's length. This is accessible via the `len` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad35418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40bbda1",
   "metadata": {},
   "source": [
    "The `.shape` attribute indicate a tensor's length along each axis. Tensors with one axis have shapes with just one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0270f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff444d5",
   "metadata": {},
   "source": [
    "To prevent confusion:\n",
    "- _order_ means the number of axes.\n",
    "- _dimensionality_ means the number of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e5986",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1634b",
   "metadata": {},
   "source": [
    "While scalars are 0th-order tensors and vectors are 1st-order tensors—matrices are 2nd-order tensors.\n",
    "\n",
    "Matrices are denoted by _bold capital_ letters (e.g., $\\textbf{X}$, $\\textbf{Y}$, and $\\textbf{Z}$).\n",
    "\n",
    "$A \\in \\mathbb{R}^{m \\times n}$ indicate that matrix $A$ contains $m \\times n$ real-valued scalars.\n",
    "- $m$ are rows.\n",
    "- $n$ are columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abfa031",
   "metadata": {},
   "source": [
    "Matrices are represented in code as tensors with two axes.\n",
    "$$\n",
    "\\begin{split}\\mathbf{A}=\n",
    "    \\begin{bmatrix} \n",
    "    a_{11} & a_{12} & \\cdots & a_{1n} \\\\ \n",
    "    a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "    a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \n",
    "    \\end{bmatrix}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We represent a matrix $A \\in \\mathbb{R}^{m \\times n}$ by a 2nd-order tensor with shape $(m, n)$.\n",
    "\n",
    "We subscript both the row and column indices to refer to an individual element $a_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f61969",
   "metadata": {},
   "source": [
    "We can convert any _appropriately_ sized $m \\times n$ tensor into an $m \\times n$ matrix using `.reshape(m, n)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e06234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3, 2)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52b84e",
   "metadata": {},
   "source": [
    "A _transposed_ matrix $A$ is denoted as $A^{\\top}$. The tranpose of an $m \\times n$ matrix is a $n \\times m$ matrix.\n",
    "\n",
    "Use `.T` to transpose any matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da273010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a4016",
   "metadata": {},
   "source": [
    "Symmetric matrices are square matrices that are equal to their own tranposes, $A = A^{\\top}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d563b510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "A == A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171119f",
   "metadata": {},
   "source": [
    "Typically, matrix rows represent individual instances and columns represent to distinct attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b8cf6",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e2953",
   "metadata": {},
   "source": [
    "Tensors provide a generic way of describing extensins to $n^\\text{th}\\text{-}$order arrays. \n",
    "\n",
    "The term _tensor_ is used in two different context:\n",
    "- Mathematical tensors are generalization of vectors and matrices representing multi-dimensional data with strict transformation rules under coordinate changes.\n",
    "- Software tensors are just multi-dimensional arrays optimized for numerical operations without adhering to the strict transformation rules.\n",
    "\n",
    "We denote general tensors by capital letters with a special font face ($\\textsf{X}$, $\\textsf{Y}$, and $\\textsf{Z}$). Their indexing follows from that of a matrix ($x_{ijk}$ and $[\\mathsf{X}]_{1, 2i-1, 3}$).\n",
    "\n",
    "\n",
    "One example of the use of tensors is with images. Each image is represented by a 3rd-order tensor with the following axes: height, width, and color channel (RGB). Additionally, a collection of images is represented in code with a 4th-order tensor where each image is indexed along the first axis.\n",
    "\n",
    "In the same vein as creating a matrix from a vector—higher order tensors are formed by growing the number of shape components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05dae03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f013996",
   "metadata": {},
   "source": [
    "## Basic Properties of Tensor Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9038190",
   "metadata": {},
   "source": [
    "Elementwise operations produce outputs that have the same shape as their operands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "916bb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()  # copy of A to B by allocating new memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49c3fc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c27f1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  2.,  4.],\n",
       "        [ 6.,  8., 10.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A+B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655616f9",
   "metadata": {},
   "source": [
    "Elementwise product of two matrices is called their Hadamard product (denoted $\\odot$). The entries of a Hadamard product of two matrices is $\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ba6092f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b4fe8",
   "metadata": {},
   "source": [
    "Adding or multiplying a tensor by a scalar produces the same shape as the tensor. Each element of the tensor is added to (or multiplied by) the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6254f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed0c2f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2,  3,  4,  5],\n",
       "         [ 6,  7,  8,  9],\n",
       "         [10, 11, 12, 13]],\n",
       "\n",
       "        [[14, 15, 16, 17],\n",
       "         [18, 19, 20, 21],\n",
       "         [22, 23, 24, 25]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19ab7992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72aa1a",
   "metadata": {},
   "source": [
    "## Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefcaca5",
   "metadata": {},
   "source": [
    "Given a vector $\\textbf{x}$ of length $n$, the sum of elements is $\\displaystyle \\sum_{i = 1}^n x_i$. This is implemented in code with the `.sum()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69a06826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8acc9ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db44c1",
   "metadata": {},
   "source": [
    "The sum of elements of an $m \\times n$ matrix $A$ is $\\displaystyle \\sum_{i = 1}^m \\sum_{j = 1}^n a_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec9ff7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa7a4e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aaed2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7c1c4",
   "metadata": {},
   "source": [
    "The sum function _reduces_ a tensor along all of its axes—producing a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a7dbe",
   "metadata": {},
   "source": [
    "We can sum over an axis by adding a keyword argument to the sum function: \n",
    "- reducing the row dimension, vertical sum: `.sum(axis=0)`.\n",
    "- reducing the column dimension, horizontal sum: `.sum(axis=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca944a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83286588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 5., 7.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=0) # vertical sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99cb31e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f627cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3., 12.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1) # horizontal sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfc7ae88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254dbb2",
   "metadata": {},
   "source": [
    "Reducing a matrix along its rows and columns is the same as summing all the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9490a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1]) == A.sum()  # Same as A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9272a",
   "metadata": {},
   "source": [
    "A _mean_, also called _average_ or _expected value_ can be  done in two ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4871cd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b82660da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d804eb",
   "metadata": {},
   "source": [
    "We can also invoke the mean function along a specific axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a7f4798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.5000, 3.5000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b351ffa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.5000, 3.5000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e1b34",
   "metadata": {},
   "source": [
    "## Non-Reduction Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc9ae26",
   "metadata": {},
   "source": [
    "Keeping the axes unchanged in reduction functions is useful for broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5df8a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3., 12.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1) # doesn't keep its axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f58e7f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d4da040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.],\n",
       "        [12.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A # keeps its axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6caeb3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a9d35",
   "metadata": {},
   "source": [
    "A use case of keeping axes unchanged in reduction is normalizing matrix rows to sum to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9826843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3333, 0.6667],\n",
       "        [0.2500, 0.3333, 0.4167]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e210d6",
   "metadata": {},
   "source": [
    "We can also do a cumulative sum of elements along an axis using the `.cumsum(axis=n)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11733826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 5., 7.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c7b90",
   "metadata": {},
   "source": [
    "## Dot Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3747ef8",
   "metadata": {},
   "source": [
    "Given two vectors $\\textbf{x}, \\textbf{y} \\in \\mathbb{R}^{d}$, the dot product $\\textbf{x}^{\\top}\\textbf{y}$ is the sum over the products of the elements at the sum position: $\\displaystyle \\textbf{x}^{\\top}\\textbf{y} = \\sum_{i=1}^d x_i y_i$\n",
    "\n",
    "The dot product is also referred as the inner product $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c69a83",
   "metadata": {},
   "source": [
    "A dot product is implemented using the `torch.dot(x, y)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd6e6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones(3, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "322ad4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb7243d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63f523d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef981c3",
   "metadata": {},
   "source": [
    "Alternatively, a dot product of two vectors can be done by implementing the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b12e702b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4965b5e",
   "metadata": {},
   "source": [
    "One use case of dot products is computing a _weighted average_.\n",
    "\n",
    "Given a vector $\\textbf{x} \\in \\mathbb{R^n}$ and weights $\\textbf{w} \\in \\mathbb{R}^n$, the weighted sum is $\\textbf{w}$ is $\\textbf{x}^\\top \\textbf{w}$.\n",
    "\n",
    "If the weights are nonnegative and sum up to 1 $\\sum_{i = 1}^n w_i = 1$, the dot product expresses a _weighted average_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128f928",
   "metadata": {},
   "source": [
    "## Matrix-Vector Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2297820",
   "metadata": {},
   "source": [
    "The product between an $m \\times n$ matrix $\\textbf{A}$ and an $n\\text{-}$dimensional vector $\\textbf{x}$ is a column vector of length $m$ whose $i^\\text{th}$ element is the doct product $a_i^{\\top}\\textbf{x}$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\\end{split}\n",
    "$$\n",
    "\n",
    "Matrix-vector products describe the key calculation involved in computing the outputs of each layer in a neural network given the ouputs from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77003f6",
   "metadata": {},
   "source": [
    "To implement a matrix-vector product in code, use the `torch.mv(A, x)` function.\n",
    "\n",
    "Note that the axis 1 (column dimension) of matrix $\\textbf{A}$ must be equal to the length of $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08c71dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape[1] == x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67734b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 14.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a3c5a",
   "metadata": {},
   "source": [
    "Python also has a _convenience_ operator `@` that is able to execute both matrix-vector and matrix-matrix products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6eeec39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 14.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c8c16",
   "metadata": {},
   "source": [
    "## Matrix-Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e37c7",
   "metadata": {},
   "source": [
    "Given two matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$, the matrix product $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$ is computed as follows.\n",
    "\n",
    "$$\n",
    "\\begin{split}\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\\end{split}\n",
    "$$\n",
    "\n",
    "_Matrix-matrix multiplication_ $\\mathbf{AB}$ can be thought of as performing the following:\n",
    "1. $m$ matrix-vector products or $m \\times n$ dot products.\n",
    "2. Combining the results together to form an $n \\times m$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961c578",
   "metadata": {},
   "source": [
    "Matrix-matrix multiplication $\\mathbf{AB}$ is implemented using the `torch.mm(A, B)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c95ba1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.ones(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8fb53eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.,  3.,  3.],\n",
       "        [12., 12., 12., 12.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75e89e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.,  3.,  3.],\n",
       "        [12., 12., 12., 12.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11dc02d",
   "metadata": {},
   "source": [
    "The term _matrix-matrix multiplication_ is often simplified to _matrix multiplication_. It should not be confused with the _Hadamard product_.\n",
    "\n",
    "While Hadamard products take quadratic time, matrix-matrix products take cubic time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c6147",
   "metadata": {},
   "source": [
    "## Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a1c1c",
   "metadata": {},
   "source": [
    "Norms are one the most useful operators in linear algebra.\n",
    "\n",
    "The norm of a vector measures its length or the magnitude of its components. It doesn't refer to its dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e34eb",
   "metadata": {},
   "source": [
    "A norm is a function $\\| \\cdot \\|$ that maps a vector to a scalar. It satisfies the following properties.\n",
    "1. Given any vector $\\textbf{x}$, if we scale all elements of the vector by a scalar $\\alpha \\in \\mathbb{R}$, the norm scales accordingly.\n",
    "\n",
    "$$\n",
    "\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|.\n",
    "$$\n",
    "\n",
    "2. For any vector $\\textbf{x}$ and $\\textbf{y}$, norms satisfy the triangle inequality.\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|.\n",
    "$$\n",
    "\n",
    "3. The norm of a vector is nonnegative and it only vanishes if the vector is zero.\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{x}\\| > 0 \\textrm{ for all } \\mathbf{x} \\neq 0.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1063b6e",
   "metadata": {},
   "source": [
    "The $l_2$ norm measures the Euclidean length of a vector. It is expressed as:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407b6bf",
   "metadata": {},
   "source": [
    "The `torch.norm(u)` method computes the $l_2$ norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eabbc7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea58aad",
   "metadata": {},
   "source": [
    "The $l_1$ norm measures the Manhattan distance. It sums up the absolute value of a vector's  elements. It is expressed as:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.\n",
    "$$\n",
    "\n",
    "It is less sensitive to outliers compared to the $l_2$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89acc7",
   "metadata": {},
   "source": [
    "The $l_1$ norm is implemented using a combination of `.abs()`. and `.sum()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e733e1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654d6a9",
   "metadata": {},
   "source": [
    "Both $l_2$ and $l_1$ are special cases of the general $l_p$ norms:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee29d2",
   "metadata": {},
   "source": [
    "The _Frobenius norm_ is defined as the square root of the sum of the squares of matrix's elements:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{X}\\|_\\textrm{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.\n",
    "$$\n",
    "\n",
    "The Frobenius norm behaves as if it were an $l_2$ norm of a matrix-shaped vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f810ee4",
   "metadata": {},
   "source": [
    "Implementing the following will compute the Frobenius norm of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80bed715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54a1c8",
   "metadata": {},
   "source": [
    "Matrices are can be seen as both collections of entries and as operators that transform vectors. \n",
    "\n",
    "For example, we can measure how much longer the matrix-vector product \n",
    "$\\textbf{Xv}$ is compared to $\\textbf{v}$, leading to the spectral norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab2388",
   "metadata": {},
   "source": [
    "While $l_2$ and $l_1$ are common vector norms, _spectral_ and _Frobenius norms_ are common matrix norms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab1787",
   "metadata": {},
   "source": [
    "In deep learning, optimization problems like maximizing the probability assigned to an observed data; or minimizing prediction errors have distances that are often expressed as norms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d45129",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2L",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
