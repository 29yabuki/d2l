{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea2e2e9",
   "metadata": {},
   "source": [
    "Derivative calculation is important to training deep networks but become are tedious and error-prone when done manually.\n",
    "These issues grow as models become more complex. Fortunately, modern deep learning frameworks handle these issues with _automatic differentiation_ (autograd).\n",
    "\n",
    "When data is passed through each successive function, the framework used builds a computational graph which tracks how values depends on others.\n",
    "\n",
    "Automatic differentiation applies the chain rule _backwards_, this algorithm called _backpropagation_.\n",
    "\n",
    "While backpropagation has become the default method for computing gradients, it is not the only option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945d49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5fe0c",
   "metadata": {},
   "source": [
    "## A simple function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa574a47",
   "metadata": {},
   "source": [
    "We differentiate a function $y = 2\\textbf{x}^{\\top}\\textbf{x}$ with respect to the column $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7609957",
   "metadata": {},
   "source": [
    "First, assign an initial value to $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398ca554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa71164",
   "metadata": {},
   "source": [
    "Allocate a place to store the gradient of $y$ with respect  to $\\textbf{x}$ to avoid allocating memory every time we take a derivative. The `requires_grad_` method signals that every operation should be tracked.\n",
    "\n",
    "The gradient of a scalar-valued function with respect to a vector $\\textbf{x}$ is vector-valued with the same shape as $\\textbf{x}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034961ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative is x = torch.arange(4.0, requires_grad=True)\n",
    "x.requires_grad_(True)\n",
    "x.grad # none by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbfe9c0",
   "metadata": {},
   "source": [
    "Compute the function of $x$ and assign it to $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175780cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d25456",
   "metadata": {},
   "source": [
    "Take the gradient of $y$ with respect to $\\textbf{x}$ using the `backward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0822b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7ee3e",
   "metadata": {},
   "source": [
    "Access the gradient using the `grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442eb372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e7867",
   "metadata": {},
   "source": [
    "We know that $y = 2\\textbf{x}^{\\top}\\textbf{x}$ with respect to $\\textbf{x}$ is $4\\textbf{x}$. \n",
    "\n",
    "Verify that both the automatic gradient computation and the expected result are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f7e81d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e65372",
   "metadata": {},
   "source": [
    "PyTorch does not automatically reset the gradient buffer when recording a new gradient. Meaning, the new gradient is added to the existing gradient.\n",
    "This is desired when optimizing the sum of multiple functions.\n",
    "\n",
    "However, when calculating for another function of $\\textbf{x}$ and taking its gradient, call the `grad.zero_` method to reset the gradient buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a068f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # resets the gradient\n",
    "y = x.sum() # compute the function of x\n",
    "y.backward() # take the gradient of y wrt to x\n",
    "x.grad # access the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7e646",
   "metadata": {},
   "source": [
    "Each term is just $x_i$ with its derivative being $\\displaystyle \\frac{dy}{dx_i} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0d468",
   "metadata": {},
   "source": [
    "## Backward for non-scalar variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c1c2d",
   "metadata": {},
   "source": [
    "When $y$ is a vector, the most natural representation of $\\displaystyle \\frac{dy}{dx}$ is matrix called the _Jacobian_.\n",
    "\n",
    "A Jacobian contains the partial derivatives of each component of $y$ with respect to each component of $x$. For higher-order $y$ and $x$, the result of differentiation is an even higher-order tensor.\n",
    "\n",
    "$$\n",
    "J =\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We use Jacobians to sum up the gradients of each component of $y$ with respect to the full vector $\\textbf{x}$—yielding a vector of the same shape as $\\textbf{x}$.\n",
    "\n",
    "When passing $\\textbf{v}^{\\top}$ as the `gradient` argument for `y.backward(gradient)`—`x.grad` will not give $J$ but $\\textbf{v}^{\\top}\\cdot J$.\n",
    "\n",
    "For instance, we often have a vector representing the value of our loss function that were calculated separately for each example among a _batch_ of training  examples. Therefore, we want to sum up the gradients computed individually for each example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2295f49c",
   "metadata": {},
   "source": [
    "Deep learning frameworks vary in how they interpret gradients of non-scalar tensors.\n",
    "\n",
    "In PyTorch, calling the `backward` method on a non-scalar produces an error unless we tell it how to reduce the object to a scalar. So, we provide some vector $\\textbf{v}$ such that `backward` will compute $\\textbf{v}^{\\top} \\partial_{\\textbf{x}}\\textbf{y}$ instead of $\\partial_{\\textbf{x}}\\textbf{y}$. Yang Zhang's [Medium post](https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29) explains this for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35b3893d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # resets the gradient\n",
    "y = x * x # computes the function of x\n",
    "v = torch.ones(len(y))\n",
    "y.backward(gradient=v)  # Faster: y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e8354",
   "metadata": {},
   "source": [
    "## Detaching computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75085609",
   "metadata": {},
   "source": [
    "Given the following equations:\n",
    "- $z = x * y$\n",
    "- $y = x * x$\n",
    "\n",
    "Manually substituting $y$ into $z$ gives:\n",
    "$$\n",
    "z = x * x^2 = x^3\n",
    "$$\n",
    "\n",
    "Computing the derivative of $z$ with respect to $x$ gives:\n",
    "$$\n",
    "\\frac{dz}{dx} = 3x^2\n",
    "$$\n",
    "\n",
    "\n",
    "Now, we introduce the variable $u = y$ but it is detached from the computational graph. Meaning, $u$ contains the same value as $y$ but PyTorch no longer tracks how it was computed from $x$—removing its history. Consequently, it prevents gradient flow.\n",
    "\n",
    "\n",
    "Since $u$ is now a constant, the function $z$ is now:\n",
    "$$\n",
    "z = x * u\n",
    "$$\n",
    "\n",
    "Deriving $z$ with respect to $x$ now becomes:\n",
    "$$\n",
    "\\frac{dz}{dx} = u = x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f8e8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # resets the gradient\n",
    "y = x * x\n",
    "u = y.detach() # detaches y\n",
    "z = u * x # computes the function of x\n",
    "\n",
    "z.sum().backward()  # takes the gradient of z wrt to x\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e93a23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True, False])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a27e9",
   "metadata": {},
   "source": [
    "Detaching $y$ removes its ancestors from the graph leading to $z$, but $y$'s graph still remains. This allows us to compute $\\nabla_x y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4faf57a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # resets the gradient\n",
    "y.sum().backward() # takes the gradient of y wrt to x\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31b51cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe48652",
   "metadata": {},
   "source": [
    "## Gradients and Python control flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90a345",
   "metadata": {},
   "source": [
    "A benefit of automatic differentiation allows us to do gradient computation in complex control flows in Python such as conditionals, loops, and function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "944f8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b79f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ccd8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e48df55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8192.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fa4db",
   "metadata": {},
   "source": [
    "$f(a)$ is a linear function of a piecewise defined  scale. Dividing $f(a)$ by $a$ gives a vector of constant entries which what represents a gradient. This is why $f(a)$ must match $\\frac{f(a)}{a}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2e3ce9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48205188",
   "metadata": {},
   "source": [
    "Dynamic control flow is commonplace in deep learning. For instance, in text processing, the computational graph depends on the length of the input. In cases like this, it is impossible to precompute gradients. Therefore, automatic differentiation is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e884a",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb29929",
   "metadata": {},
   "source": [
    "Remember these basics:\n",
    "1. Attach gradients to those variables with respect to which we desire derivatives.\n",
    "2. Record the computation of the target value.\n",
    "3. Execute the backpropagation function.\n",
    "4. Access the resulting gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd0df8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True) # step 1\n",
    "y = 2 * torch.dot(x, x) # step 2\n",
    "y.backward() # step 3\n",
    "x.grad # step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07ee5e",
   "metadata": {},
   "source": [
    "##  Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfc564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2L",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
